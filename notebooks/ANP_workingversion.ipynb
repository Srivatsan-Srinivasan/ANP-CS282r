{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "eQ7K9blSUKTs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "# (Attentive) Neural Processes for 1D regression\n",
        "\n",
        "Regression is usually cast as modelling the distribution of output **y** given input **x** via a deterministic function, such as a neural network, taking **x** as input. In this setting, the model is trained on a dataset of input-output pairs, and predictions of the outputs are independent of each other given the inputs. An alternative approach to regression involves using the training data to compute a distribution over functions that map inputs to outputs, and using draws from that distribution to make predictions on test inputs. This approach allows for reasoning about multiple functions consistent with the data, and can capture the co-variability in outputs given inputs. In the Bayesian machine learning literature, non-parametric models such as Gaussian Processes (GPs) are popular choices of this approach.\n",
        "\n",
        "[Neural Processes](https://arxiv.org/abs/1807.01622) (NPs) also approach regression by modelling a distribution over regression functions. Each function models the distribution of the output given an input, conditioning on some observed input-output pairs, which we call the context. Modelling this distribution over functions was made possible by incorporating a latent variable to the [Conditional Neural Process](https://arxiv.org/abs/1807.01613) (CNP). \n",
        "\n",
        "However NPs suffer from underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. Hence [Attentive Neural Processes](https://arxiv.org/abs/1901.05761) (ANPs) were introduced, addressing this issue by incorporating attention into NPs. We share the implementation of ANPs (and NPs, which is a special case of ANP with uniform attention) for a 1D regression task where (A)NPs are trained on random 1D functions."
      ]
    },
    {
      "metadata": {
        "id": "UMJxjfsTa08h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "Ncb7M0FpNfix",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ARGYAmEsa5K7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data generator\n",
        "Instead of training using observations from a single function (as in classic regression tasks), we would like to train on a dataset that comes from multiple functions with shared characteristics. Hence for training, we use data that comes from a Gaussian Process (GP) with randomly varying kernel parameters.  At each training iteration, we sample a batch of random kernel parameters, and for each parameter setting we sample a curve (a realisation) from the corresponding GP. We select random points on each curve to be the targets and a subset to be the contexts for optimising the training loss. The data generation is almost the same as for the [implementation of CNPs](https://github.com/deepmind/conditional-neural-process/blob/master/conditional_neural_process.ipynb), but with kernel parameters varying randomly at each iteration."
      ]
    },
    {
      "metadata": {
        "id": "Px-atGEfNnWT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# The (A)NP takes as input a `NPRegressionDescription` namedtuple with fields:\n",
        "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
        "#   `target_y`: a tensor containing the ground truth for the targets to be\n",
        "#     predicted\n",
        "#   `num_total_points`: A vector containing a scalar that describes the total\n",
        "#     number of datapoints used (context + target)\n",
        "#   `num_context_points`: A vector containing a scalar that describes the number\n",
        "#     of datapoints used as context\n",
        "# The GPCurvesReader returns the newly sampled data in this format at each\n",
        "# iteration\n",
        "\n",
        "NPRegressionDescription = collections.namedtuple(\n",
        "    \"NPRegressionDescription\",\n",
        "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
        "\n",
        "\n",
        "class GPCurvesReader(object):\n",
        "  \"\"\"Generates curves using a Gaussian Process (GP).\n",
        "\n",
        "  Supports vector inputs (x) and vector outputs (y). Kernel is\n",
        "  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
        "  some factor chosen randomly in a range. Outputs are independent gaussian\n",
        "  processes.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               batch_size,\n",
        "               max_num_context,\n",
        "               x_size=1,\n",
        "               y_size=1,\n",
        "               l1_scale=0.6,\n",
        "               sigma_scale=1.0,\n",
        "               random_kernel_parameters=True,\n",
        "               testing=False):\n",
        "    \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
        "\n",
        "    Args:\n",
        "      batch_size: An integer.\n",
        "      max_num_context: The max number of observations in the context.\n",
        "      x_size: Integer >= 1 for length of \"x values\" vector.\n",
        "      y_size: Integer >= 1 for length of \"y values\" vector.\n",
        "      l1_scale: Float; typical scale for kernel distance function.\n",
        "      sigma_scale: Float; typical scale for variance.\n",
        "      random_kernel_parameters: If `True`, the kernel parameters (l1 and sigma) \n",
        "          will be sampled uniformly within [0.1, l1_scale] and [0.1, sigma_scale].\n",
        "      testing: Boolean that indicates whether we are testing. If so there are\n",
        "          more targets for visualization.\n",
        "    \"\"\"\n",
        "    self._batch_size = batch_size\n",
        "    self._max_num_context = max_num_context\n",
        "    self._x_size = x_size\n",
        "    self._y_size = y_size\n",
        "    self._l1_scale = l1_scale\n",
        "    self._sigma_scale = sigma_scale\n",
        "    self._random_kernel_parameters = random_kernel_parameters\n",
        "    self._testing = testing\n",
        "\n",
        "  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
        "    \"\"\"Applies the Gaussian kernel to generate curve data.\n",
        "\n",
        "    Args:\n",
        "      xdata: Tensor of shape [B, num_total_points, x_size] with\n",
        "          the values of the x-axis data.\n",
        "      l1: Tensor of shape [B, y_size, x_size], the scale\n",
        "          parameter of the Gaussian kernel.\n",
        "      sigma_f: Tensor of shape [B, y_size], the magnitude\n",
        "          of the std.\n",
        "      sigma_noise: Float, std of the noise that we add for stability.\n",
        "\n",
        "    Returns:\n",
        "      The kernel, a float tensor of shape\n",
        "      [B, y_size, num_total_points, num_total_points].\n",
        "    \"\"\"\n",
        "    num_total_points = tf.shape(xdata)[1]\n",
        "\n",
        "    # Expand and take the difference\n",
        "    xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
        "    xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
        "    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
        "\n",
        "    # [B, y_size, num_total_points, num_total_points, x_size]\n",
        "    norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
        "\n",
        "    norm = tf.reduce_sum(\n",
        "        norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
        "\n",
        "    # [B, y_size, num_total_points, num_total_points]\n",
        "    kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n",
        "\n",
        "    # Add some noise to the diagonal to make the cholesky work.\n",
        "    kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
        "\n",
        "    return kernel\n",
        "\n",
        "  def generate_curves(self):\n",
        "    \"\"\"Builds the op delivering the data.\n",
        "\n",
        "    Generated functions are `float32` with x values between -2 and 2.\n",
        "    \n",
        "    Returns:\n",
        "      A `CNPRegressionDescription` namedtuple.\n",
        "    \"\"\"\n",
        "    num_context = tf.random_uniform(\n",
        "        shape=[], minval=3, maxval=self._max_num_context, dtype=tf.int32)\n",
        "\n",
        "    # If we are testing we want to have more targets and have them evenly\n",
        "    # distributed in order to plot the function.\n",
        "    if self._testing:\n",
        "      num_target = 400\n",
        "      num_total_points = num_target\n",
        "      x_values = tf.tile(\n",
        "          tf.expand_dims(tf.range(-2., 2., 1. / 100, dtype=tf.float32), axis=0),\n",
        "          [self._batch_size, 1])\n",
        "      x_values = tf.expand_dims(x_values, axis=-1)\n",
        "    # During training the number of target points and their x-positions are\n",
        "    # selected at random\n",
        "    else:\n",
        "      num_target = tf.random_uniform(shape=(), minval=0, \n",
        "                                     maxval=self._max_num_context - num_context,\n",
        "                                     dtype=tf.int32)\n",
        "      num_total_points = num_context + num_target\n",
        "      x_values = tf.random_uniform(\n",
        "          [self._batch_size, num_total_points, self._x_size], -2, 2)\n",
        "\n",
        "    # Set kernel parameters\n",
        "    # Either choose a set of random parameters for the mini-batch\n",
        "    if self._random_kernel_parameters:\n",
        "      l1 = tf.random_uniform([self._batch_size, self._y_size,\n",
        "                              self._x_size], 0.1, self._l1_scale)\n",
        "      sigma_f = tf.random_uniform([self._batch_size, self._y_size],\n",
        "                                  0.1, self._sigma_scale)\n",
        "    # Or use the same fixed parameters for all mini-batches\n",
        "    else:\n",
        "      l1 = tf.ones(shape=[self._batch_size, self._y_size,\n",
        "                          self._x_size]) * self._l1_scale\n",
        "      sigma_f = tf.ones(shape=[self._batch_size,\n",
        "                               self._y_size]) * self._sigma_scale\n",
        "\n",
        "    # Pass the x_values through the Gaussian kernel\n",
        "    # [batch_size, y_size, num_total_points, num_total_points]\n",
        "    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
        "\n",
        "    # Calculate Cholesky, using double precision for better stability:\n",
        "    cholesky = tf.cast(tf.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n",
        "\n",
        "    # Sample a curve\n",
        "    # [batch_size, y_size, num_total_points, 1]\n",
        "    y_values = tf.matmul(\n",
        "        cholesky,\n",
        "        tf.random_normal([self._batch_size, self._y_size, num_total_points, 1]))\n",
        "\n",
        "    # [batch_size, num_total_points, y_size]\n",
        "    y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n",
        "\n",
        "    if self._testing:\n",
        "      # Select the targets\n",
        "      target_x = x_values\n",
        "      target_y = y_values\n",
        "\n",
        "      # Select the observations\n",
        "      idx = tf.random_shuffle(tf.range(num_target))\n",
        "      context_x = tf.gather(x_values, idx[:num_context], axis=1)\n",
        "      context_y = tf.gather(y_values, idx[:num_context], axis=1)\n",
        "\n",
        "    else:\n",
        "      # Select the targets which will consist of the context points as well as\n",
        "      # some new target points\n",
        "      target_x = x_values[:, :num_target + num_context, :]\n",
        "      target_y = y_values[:, :num_target + num_context, :]\n",
        "\n",
        "      # Select the observations\n",
        "      context_x = x_values[:, :num_context, :]\n",
        "      context_y = y_values[:, :num_context, :]\n",
        "\n",
        "    query = ((context_x, context_y), target_x)\n",
        "\n",
        "    return NPRegressionDescription(\n",
        "        query=query,\n",
        "        target_y=target_y,\n",
        "        num_total_points=tf.shape(target_x)[1],\n",
        "        num_context_points=num_context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRTKMWLHVBMq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# periodic kernel\n",
        "#https://www.cs.toronto.edu/~duvenaud/cookbook/\n",
        "\n",
        "NPRegressionDescription = collections.namedtuple(\n",
        "    \"NPRegressionDescription\",\n",
        "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
        "\n",
        "\n",
        "class GPSinCurvesReader(object):\n",
        "  \"\"\"Generates curves using a Gaussian Process (GP).\n",
        "\n",
        "  Supports vector inputs (x) and vector outputs (y). Kernel is\n",
        "  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
        "  some factor chosen randomly in a range. Outputs are independent gaussian\n",
        "  processes.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               batch_size,\n",
        "               max_num_context,\n",
        "               x_size=1,\n",
        "               y_size=1,\n",
        "               l1_scale=0.6,\n",
        "               sigma_scale=1.0,\n",
        "               random_kernel_parameters=True,\n",
        "               testing=False):\n",
        "    \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
        "\n",
        "    Args:\n",
        "      batch_size: An integer.\n",
        "      max_num_context: The max number of observations in the context.\n",
        "      x_size: Integer >= 1 for length of \"x values\" vector.\n",
        "      y_size: Integer >= 1 for length of \"y values\" vector.\n",
        "      l1_scale: Float; typical scale for kernel distance function.\n",
        "      sigma_scale: Float; typical scale for variance.\n",
        "      random_kernel_parameters: If `True`, the kernel parameters (l1 and sigma) \n",
        "          will be sampled uniformly within [0.1, l1_scale] and [0.1, sigma_scale].\n",
        "      testing: Boolean that indicates whether we are testing. If so there are\n",
        "          more targets for visualization.\n",
        "    \"\"\"\n",
        "    self._batch_size = batch_size\n",
        "    self._max_num_context = max_num_context\n",
        "    self._x_size = x_size\n",
        "    self._y_size = y_size\n",
        "    self._l1_scale = l1_scale\n",
        "    self._sigma_scale = sigma_scale\n",
        "    self._random_kernel_parameters = random_kernel_parameters\n",
        "    self._testing = testing\n",
        "\n",
        "  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
        "    \"\"\"Applies the Gaussian kernel to generate curve data.\n",
        "\n",
        "    Args:\n",
        "      xdata: Tensor of shape [B, num_total_points, x_size] with\n",
        "          the values of the x-axis data.\n",
        "      l1: Tensor of shape [B, y_size, x_size], the scale\n",
        "          parameter of the Gaussian kernel.\n",
        "      sigma_f: Tensor of shape [B, y_size], the magnitude\n",
        "          of the std.\n",
        "      sigma_noise: Float, std of the noise that we add for stability.\n",
        "\n",
        "    Returns:\n",
        "      The kernel, a float tensor of shape\n",
        "      [B, y_size, num_total_points, num_total_points].\n",
        "    \"\"\"\n",
        "    num_total_points = tf.shape(xdata)[1]\n",
        "\n",
        "    # Expand and take the difference\n",
        "    xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
        "    xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
        "    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
        "\n",
        "    # [B, y_size, num_total_points, num_total_points, x_size]\n",
        "    norm = 2*tf.square(tf.math.sin(3.14*diff[:, None, :, :, :])) / l1[:, :, None, None, :]\n",
        "\n",
        "    norm = tf.reduce_sum(\n",
        "        norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
        "\n",
        "    # [B, y_size, num_total_points, num_total_points]\n",
        "    kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-norm)\n",
        "\n",
        "    # Add some noise to the diagonal to make the cholesky work.\n",
        "    kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
        "\n",
        "    return kernel\n",
        "\n",
        "  def generate_curves(self):\n",
        "    \"\"\"Builds the op delivering the data.\n",
        "\n",
        "    Generated functions are `float32` with x values between -2 and 2.\n",
        "    \n",
        "    Returns:\n",
        "      A `CNPRegressionDescription` namedtuple.\n",
        "    \"\"\"\n",
        "    num_context = tf.random_uniform(\n",
        "        shape=[], minval=3, maxval=self._max_num_context, dtype=tf.int32)\n",
        "\n",
        "    # If we are testing we want to have more targets and have them evenly\n",
        "    # distributed in order to plot the function.\n",
        "    if self._testing:\n",
        "      num_target = 400\n",
        "      num_total_points = num_target\n",
        "      x_values = tf.tile(\n",
        "          tf.expand_dims(tf.range(-2., 2., 1. / 100, dtype=tf.float32), axis=0),\n",
        "          [self._batch_size, 1])\n",
        "      x_values = tf.expand_dims(x_values, axis=-1)\n",
        "    # During training the number of target points and their x-positions are\n",
        "    # selected at random\n",
        "    else:\n",
        "      num_target = tf.random_uniform(shape=(), minval=0, \n",
        "                                     maxval=self._max_num_context - num_context,\n",
        "                                     dtype=tf.int32)\n",
        "      num_total_points = num_context + num_target\n",
        "      x_values = tf.random_uniform(\n",
        "          [self._batch_size, num_total_points, self._x_size], -2, 2)\n",
        "\n",
        "    # Set kernel parameters\n",
        "    # Either choose a set of random parameters for the mini-batch\n",
        "    if self._random_kernel_parameters:\n",
        "      l1 = tf.random_uniform([self._batch_size, self._y_size,\n",
        "                              self._x_size], 0.1, self._l1_scale)\n",
        "      sigma_f = tf.random_uniform([self._batch_size, self._y_size],\n",
        "                                  0.1, self._sigma_scale)\n",
        "    # Or use the same fixed parameters for all mini-batches\n",
        "    else:\n",
        "      l1 = tf.ones(shape=[self._batch_size, self._y_size,\n",
        "                          self._x_size]) * self._l1_scale\n",
        "      sigma_f = tf.ones(shape=[self._batch_size,\n",
        "                               self._y_size]) * self._sigma_scale\n",
        "\n",
        "    # Pass the x_values through the Gaussian kernel\n",
        "    # [batch_size, y_size, num_total_points, num_total_points]\n",
        "    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
        "\n",
        "    # Calculate Cholesky, using double precision for better stability:\n",
        "    cholesky = tf.cast(tf.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n",
        "\n",
        "    # Sample a curve\n",
        "    # [batch_size, y_size, num_total_points, 1]\n",
        "    y_values = tf.matmul(\n",
        "        cholesky,\n",
        "        tf.random_normal([self._batch_size, self._y_size, num_total_points, 1]))\n",
        "\n",
        "    # [batch_size, num_total_points, y_size]\n",
        "    y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n",
        "\n",
        "    if self._testing:\n",
        "      # Select the targets\n",
        "      target_x = x_values\n",
        "      target_y = y_values\n",
        "\n",
        "      # Select the observations\n",
        "      idx = tf.random_shuffle(tf.range(num_target))\n",
        "      context_x = tf.gather(x_values, idx[:num_context], axis=1)\n",
        "      context_y = tf.gather(y_values, idx[:num_context], axis=1)\n",
        "\n",
        "    else:\n",
        "      # Select the targets which will consist of the context points as well as\n",
        "      # some new target points\n",
        "      target_x = x_values[:, :num_target + num_context, :]\n",
        "      target_y = y_values[:, :num_target + num_context, :]\n",
        "\n",
        "      # Select the observations\n",
        "      context_x = x_values[:, :num_context, :]\n",
        "      context_y = y_values[:, :num_context, :]\n",
        "\n",
        "    query = ((context_x, context_y), target_x)\n",
        "\n",
        "    return NPRegressionDescription(\n",
        "        query=query,\n",
        "        target_y=target_y,\n",
        "        num_total_points=tf.shape(target_x)[1],\n",
        "        num_context_points=num_context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GhMwui0VfmNn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attentive Neural Processes: a short introduction\n",
        "\n",
        "Here below are the model diagrams for the **NP** (left) and **ANP** (right).\n",
        "\n",
        "![](https://i.ibb.co/Js1B7RB/model-figure-new-1-page-001.jpg)\n",
        "\n",
        "In the NP, the **context points** $(x_C,y_C)=(x_i, y_i)_{i \\in C}$ are passed through the encoder that consists of two paths, a **deterministic path** and and a **latent path**. \n",
        "\n",
        "In the **deterministic path**, each context pair $(x_i,y_i)$ is passed through an MLP (shared parameters across the contexts) to produce representation $r_i$. These are aggregated by taking the mean to produce the deterministic code $r_C$. \n",
        "\n",
        "In the **latent path**, a code $s_C$ is computed in a similar manner from the representations $s_i$, and is used to parameterise the distribution of the latent variable $z$, giving the latent distribution $q(z|s_C)$.\n",
        "\n",
        "In the decoder, the $r_C$ and $z$ are concatenated alongside $x_*$ and passed through an MLP to produce the parameters of the distribution $p(y_*|x_*,r_C,z)$. \n",
        "\n",
        "The motivation for having a global latent is to model different realisations of the data generating stochastic process - each sample of $z$ would correspond to one realisation of the stochastic process. One can define the model using just the deterministic path, just the latent path, or both.\n",
        "\n",
        "One problem of the NP is that the **mean-aggregation step in the encoder acts as a bottleneck**: since taking the mean across context representations gives the same weight to each context point, it is difficult for the decoder to learn which context points provide relevant information for a given target prediction. \n",
        "\n",
        "This is addressed by ANPs, where the mean-aggregation is replaced by a **cross-attention mechanism** - the target query $x_*$ attends to the key-value pairs $(x_i,r_i)_{i \\in C}$ and assigns weights $w_i$ to each pair to form a query-specific representation $r_*=\\sum_i w_i r_i$. This is precisely where the model allows each query to attend more closely to the context points that it deems relevant for the prediction. Note that if we use uniform attention (all $w_i$ equal), then we revert to the NP.\n",
        "\n",
        "Another change is the **self-attention mechanism** that replaces the MLPs in the encoder, used in order to model interactions between the context points. However for the 1D regression task here, we do not use self-attention and resort to the MLP setting as it is shown to be sufficient, and just use cross-attention.\n",
        "\n",
        "Learning for both the NP and ANP is done by optimising the ELBO to the log predictive likelihood:\n",
        "\n",
        "$$\\log p(y_T|x_T,x_C,y_C)  \\geq\n",
        "\\mathbb{E}_{q(z|s_T)}  [\\log p(y_T|x_T,r_C,z)] - KL ( q(z|s_T) \\Vert q(z|s_C) )$$\n",
        "where $C$ represents contexts and $T$ represents targets."
      ]
    },
    {
      "metadata": {
        "id": "ps97odopnvkv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# utility methods\n",
        "def batch_mlp(input, output_sizes, variable_scope):\n",
        "  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n",
        "  \n",
        "  Args:\n",
        "    input: input tensor of shape [B,n,d_in].\n",
        "    output_sizes: An iterable containing the output sizes of the MLP as defined \n",
        "        in `basic.Linear`.\n",
        "    variable_scope: String giving the name of the variable scope. If this is set\n",
        "        to be the same as a previously defined MLP, then the weights are reused.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n",
        "  \"\"\"\n",
        "  # Get the shapes of the input and reshape to parallelise across observations\n",
        "  batch_size, _, filter_size = input.shape.as_list()\n",
        "  output = tf.reshape(input, (-1, filter_size))\n",
        "  output.set_shape((None, filter_size))\n",
        "\n",
        "  # Pass through MLP\n",
        "  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
        "    for i, size in enumerate(output_sizes[:-1]):\n",
        "      output = tf.nn.relu(\n",
        "          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n",
        "\n",
        "    # Last layer without a ReLu\n",
        "    output = tf.layers.dense(\n",
        "        output, output_sizes[-1], name=\"layer_{}\".format(i + 1))\n",
        "\n",
        "  # Bring back into original shape\n",
        "  output = tf.reshape(output, (batch_size, -1, output_sizes[-1]))\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kli9cfXqt2Jf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder: Deterministic Path\n",
        "\n",
        "The encoder in the deterministic path is shared between all context pairs and consists of an MLP and an attention module. Each context $x_i$ and $y_i$ are concatenated and passed through the MLP (with relu non-linearities) to output a representation $r_i$. These $(r_i)_{i \\in C}$ and $x_i$ are fed into the cross-attention module, along with query $x_*$ to output a query-specific representation $r_*$. The MLP architecture is given by the `output_sizes` argument, a list of hidden layer sizes, and the `attention` argument which is the cross-attention module defined later on."
      ]
    },
    {
      "metadata": {
        "id": "yXdB68gkPwy2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeterministicEncoder(object):\n",
        "  \"\"\"The Deterministic Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, attention, use_self_attention = False):\n",
        "    \"\"\"(A)NP deterministic encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      attention: The attention module.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    self._attention = attention\n",
        "    \n",
        "    self._use_self_attention = use_self_attention\n",
        "\n",
        "  def __call__(self, context_x, context_y, target_x):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      context_x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      context_y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "      target_x: Tensor of shape [B,target_observations,d_x]. \n",
        "          For this 1D regression task this corresponds to the x-values.\n",
        "\n",
        "    Returns:\n",
        "      The encoded representation. Tensor of shape [B,target_observations,d]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([context_x, context_y], axis=-1)\n",
        "    \n",
        "    if self._use_self_attention:\n",
        "      print('Uaing self attention in the deterministic encoder')\n",
        "      encoder_input = batch_mlp(encoder_input, [2,128,128],\"deterministic_encoder_self\")  \n",
        "\n",
        "      with tf.variable_scope(\"deterministic_encoder_self\", reuse=tf.AUTO_REUSE):\n",
        "        encoder_input = self._attention(encoder_input,encoder_input,encoder_input)\n",
        "        \n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(encoder_input, self._output_sizes, \n",
        "                       \"deterministic_encoder\")\n",
        "\n",
        "    # Apply attention\n",
        "    with tf.variable_scope(\"deterministic_encoder\", reuse=tf.AUTO_REUSE):\n",
        "        hidden = self._attention(context_x, target_x, hidden)\n",
        "\n",
        "    return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1944scGst6fS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder: Latent Path\n",
        "\n",
        "The encoder in the latent path is again shared by all context pairs and consists of an MLP.  After the MLP (with relu non-linearities) is applied the resulting representation is aggregated by taking the mean across the contexts and a further MLP is applied to compute the mean and variance of the Gaussian $q(z|s_C)$. Again the initial MLP's architecture is given by the `output_sizes` argument, and the `num_latent` argument sets the latent dimensionality."
      ]
    },
    {
      "metadata": {
        "id": "gufhxfGSRCs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LatentEncoder(object):\n",
        "  \"\"\"The Latent Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, num_latents, attention, use_self_attention):\n",
        "    \"\"\"(A)NP latent encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      num_latents: The latent dimensionality.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    self._num_latents = num_latents\n",
        "    self._attention = attention\n",
        "    \n",
        "    self._use_self_attention = use_self_attention\n",
        "\n",
        "  def __call__(self, x, y):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "\n",
        "    Returns:\n",
        "      A normal distribution over tensors of shape [B, num_latents]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([x, y], axis=-1)\n",
        "    \n",
        "    if self._use_self_attention:\n",
        "      print('Using self attention in the latent encoder')\n",
        "      encoder_input = batch_mlp(encoder_input, [2,128,128],\"latent_encoder_self\")  \n",
        "\n",
        "      with tf.variable_scope(\"latent_encoder_self\", reuse=tf.AUTO_REUSE):\n",
        "        encoder_input = self._attention(encoder_input,encoder_input,encoder_input)\n",
        "    \n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(encoder_input, self._output_sizes, \"latent_encoder\")\n",
        "    \n",
        "    # Aggregator: take the mean over all points\n",
        "    hidden = tf.reduce_mean(hidden, axis=1)\n",
        "    \n",
        "    # Have further MLP layers that map to the parameters of the Gaussian latent\n",
        "    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n",
        "      # First apply intermediate relu layer \n",
        "      hidden = tf.nn.relu(\n",
        "          tf.layers.dense(hidden, \n",
        "                          (self._output_sizes[-1] + self._num_latents)/2, \n",
        "                          name=\"penultimate_layer\"))\n",
        "      # Then apply further linear layers to output latent mu and log sigma\n",
        "      mu = tf.layers.dense(hidden, self._num_latents, name=\"mean_layer\")\n",
        "      log_sigma = tf.layers.dense(hidden, self._num_latents, name=\"std_layer\")\n",
        "      \n",
        "    # Compute sigma\n",
        "    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n",
        "\n",
        "    return tf.contrib.distributions.Normal(loc=mu, scale=sigma)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "of3idijgOnpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: add self-attention as an option\n",
        "\n",
        "class LatentEncoder_cross(object):\n",
        "  \"\"\"The Latent Encoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, num_latents,attention,use_self_attention):\n",
        "    \"\"\"(A)NP latent encoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
        "      num_latents: The latent dimensionality.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "    print(\"Using cross attention in the latent encoder \")\n",
        "    self._num_latents = num_latents\n",
        "    self._attention = attention\n",
        "    \n",
        "    self._use_self_attention = use_self_attention\n",
        "\n",
        "  def __call__(self, x_cont, y_cont, x_targ):\n",
        "    \"\"\"Encodes the inputs into one representation.\n",
        "\n",
        "    Args:\n",
        "      x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
        "          task this corresponds to the x-values.\n",
        "      y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
        "          task this corresponds to the y-values.\n",
        "\n",
        "    Returns:\n",
        "      A normal distribution over tensors of shape [B, num_latents]\n",
        "    \"\"\"\n",
        "\n",
        "    # Concatenate x and y along the filter axes\n",
        "    encoder_input = tf.concat([x_cont, y_cont], axis=-1)\n",
        "    \n",
        "    if self._use_self_attention:\n",
        "      print('Using self attention in the latent cross encoder')\n",
        "      encoder_input = batch_mlp(encoder_input, [2,128,128],\"latent_encoder_cross_self\")  \n",
        "\n",
        "      with tf.variable_scope(\"latent_encoder_cross_self\", reuse=tf.AUTO_REUSE):\n",
        "        encoder_input = self._attention(encoder_input,encoder_input,encoder_input)\n",
        "    \n",
        "    # Pass final axis through MLP\n",
        "    hidden_mean = batch_mlp(encoder_input, [2,self._num_latents], \"latent_encode_cross_mean\")\n",
        "    hidden_var = batch_mlp(encoder_input, [2,self._num_latents], \"latent_encode_cross_var\")\n",
        "    \n",
        "    sigma = 0.1 + 0.9 * tf.sigmoid(hidden_var)\n",
        "    \n",
        "    z_samps = tf.contrib.distributions.Normal(loc = hidden_mean, scale = sigma).sample()\n",
        "    \n",
        "    with tf.variable_scope(\"latent_cross\", reuse=tf.AUTO_REUSE):        \n",
        "        z_star = self._attention(x_cont,x_targ,z_samps)\n",
        "    \n",
        "    return z_star, z_samps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "roGKUH3Nt9Xg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "\n",
        "The context representation (either $z$ or $z$ and $r_C$ concatenated) and the target inputs $x_T$ are fed into the decoder. First they are concatenated and passed through an MLP, whose architecture is given by the `output_sizes` argument. The MLP outputs the mean and variance of the Gaussian $p(y_T|x_T,r_C,z)$. "
      ]
    },
    {
      "metadata": {
        "id": "RLhx-J8ALyST",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class Decoder(object):\n",
        "  \"\"\"The Decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, output_sizes, apply_attention=True, attention = None):\n",
        "    \"\"\"(A)NP decoder.\n",
        "\n",
        "    Args:\n",
        "      output_sizes: An iterable containing the output sizes of the decoder MLP \n",
        "          as defined in `basic.Linear`.\n",
        "    \"\"\"\n",
        "    self._output_sizes = output_sizes\n",
        "   \n",
        "    self._apply_attention = apply_attention   \n",
        "    \n",
        "    if self._apply_attention:\n",
        "      self._attention = attention\n",
        "          \n",
        "    print('Decoder Attention is ', self._apply_attention)\n",
        "    \n",
        "    \n",
        "\n",
        "  def __call__(self, representation, target_x):\n",
        "    \"\"\"Decodes the individual targets.\n",
        "\n",
        "    Args:\n",
        "      representation: The representation of the context for target predictions. \n",
        "          Tensor of shape [B,target_observations,?].\n",
        "      target_x: The x locations for the target query.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "\n",
        "    Returns:\n",
        "      dist: A multivariate Gaussian over the target points. A distribution over\n",
        "          tensors of shape [B,target_observations,d_y].\n",
        "      mu: The mean of the multivariate Gaussian.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "      sigma: The standard deviation of the multivariate Gaussian.\n",
        "          Tensor of shape [B,target_observations,d_x].\n",
        "    \"\"\"\n",
        "   \n",
        "    \n",
        "    if self._apply_attention:\n",
        "      print('Using self attention in the decoder')\n",
        "      hidden_decoder = batch_mlp(target_x, [1,128,128],\"hidden_decoder\")    \n",
        "      print(\"Input Hidden \", hidden_decoder)\n",
        "      with tf.variable_scope(\"hidden_decoder\", reuse=tf.AUTO_REUSE):        \n",
        "        hidden_decoder = self._attention(hidden_decoder ,hidden_decoder,hidden_decoder)\n",
        "      print(\"Attention Hidden \", hidden_decoder)     \n",
        "    \n",
        "      hidden = tf.concat([representation, hidden_decoder], axis=-1)\n",
        "    else:\n",
        "      hidden = tf.concat([representation, target_x], axis=-1)\n",
        "      \n",
        "    \n",
        "    # Pass final axis through MLP\n",
        "    hidden = batch_mlp(hidden, self._output_sizes, \"decoder\")\n",
        "    print(\"Output hidden \", hidden)\n",
        "   \n",
        "   \n",
        "    # Get the mean an the variance\n",
        "    mu, log_sigma = tf.split(hidden, 2, axis=-1)\n",
        "\n",
        "    # Bound the variance\n",
        "    sigma = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
        "\n",
        "    # Get the distribution\n",
        "    dist = tf.contrib.distributions.MultivariateNormalDiag(\n",
        "        loc=mu, scale_diag=sigma)\n",
        "\n",
        "    return dist, mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bOfbRLHpt_KK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "We can put the encoders and the decoder together to form the ANP model."
      ]
    },
    {
      "metadata": {
        "id": "n5XJ6gr7ZDPl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LatentModel(object):\n",
        "  \"\"\"The (A)NP model.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_encoder_output_sizes, num_latents,\n",
        "               decoder_output_sizes, use_deterministic_path=True, \n",
        "               deterministic_encoder_output_sizes=None, attention=None, \n",
        "               use_attention_decoder=False,\n",
        "               use_encoder_determ_self_attention = False,\n",
        "               use_encoder_latent_self_attention = False,\n",
        "               use_encoder_latent_cross_attention = False):\n",
        "    \"\"\"Initialises the model.\n",
        "\n",
        "    Args:\n",
        "      latent_encoder_output_sizes: An iterable containing the sizes of hidden \n",
        "          layers of the latent encoder.\n",
        "      num_latents: The latent dimensionality.\n",
        "      decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
        "          the decoder. The last element should correspond to d_y * 2\n",
        "          (it encodes both mean and variance concatenated)\n",
        "      use_deterministic_path: a boolean that indicates whether the deterministic\n",
        "          encoder is used or not.\n",
        "      deterministic_encoder_output_sizes: An iterable containing the sizes of \n",
        "          hidden layers of the deterministic encoder. The last one is the size \n",
        "          of the deterministic representation r.\n",
        "      attention: The attention module used in the deterministic encoder.\n",
        "          Only relevant when use_deterministic_path=True.\n",
        "    \"\"\"\n",
        "    self._latent_encoder = LatentEncoder(latent_encoder_output_sizes, \n",
        "                                         num_latents,attention, use_encoder_latent_self_attention)\n",
        "    self._use_encoder_latent_cross_attention = use_encoder_latent_cross_attention\n",
        "    if use_encoder_latent_cross_attention:\n",
        "      self._latent_encoder_cross = LatentEncoder_cross(latent_encoder_output_sizes,\n",
        "                                                    num_latents,attention,use_encoder_latent_self_attention)\n",
        "    self._decoder = Decoder(decoder_output_sizes, apply_attention = use_attention_decoder, attention = attention)\n",
        "    self._use_deterministic_path = use_deterministic_path\n",
        "    if use_deterministic_path:\n",
        "      self._deterministic_encoder = DeterministicEncoder(\n",
        "          deterministic_encoder_output_sizes, attention, use_encoder_determ_self_attention)\n",
        "    \n",
        "  def __call__(self, query, num_targets, target_y=None):\n",
        "    \"\"\"Returns the predicted mean and variance at the target points.\n",
        "\n",
        "    Args:\n",
        "      query: Array containing ((context_x, context_y), target_x) where:\n",
        "          context_x: Tensor of shape [B,num_contexts,d_x]. \n",
        "              Contains the x values of the context points.\n",
        "          context_y: Tensor of shape [B,num_contexts,d_y]. \n",
        "              Contains the y values of the context points.\n",
        "          target_x: Tensor of shape [B,num_targets,d_x]. \n",
        "              Contains the x values of the target points.\n",
        "      num_targets: Number of target points.\n",
        "      target_y: The ground truth y values of the target y. \n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "\n",
        "    Returns:\n",
        "      log_p: The log_probability of the target_y given the predicted\n",
        "          distribution. Tensor of shape [B,num_targets].\n",
        "      mu: The mean of the predicted distribution. \n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "      sigma: The variance of the predicted distribution.\n",
        "          Tensor of shape [B,num_targets,d_y].\n",
        "    \"\"\"\n",
        "\n",
        "    (context_x, context_y), target_x = query\n",
        "\n",
        "    # Pass query through the encoder and the decoder\n",
        "    prior = self._latent_encoder(context_x, context_y)\n",
        "    if self._use_encoder_latent_cross_attention:\n",
        "      latent_cross,z_samps_prior = self._latent_encoder_cross(context_x, context_y, target_x)\n",
        "    \n",
        "    # For training, when target_y is available, use targets for latent encoder.\n",
        "    # Note that targets contain contexts by design.\n",
        "    if target_y is None:\n",
        "      latent_rep = prior.sample()\n",
        "    # For testing, when target_y unavailable, use contexts for latent encoder.\n",
        "    else:\n",
        "      posterior = self._latent_encoder(target_x, target_y)\n",
        "      latent_rep = posterior.sample()\n",
        "      if self._use_encoder_latent_cross_attention:\n",
        "        latent_cross,z_samps_posterior = self._latent_encoder_cross(target_x, target_y, target_x) \n",
        "    \n",
        "    latent_rep = tf.tile(tf.expand_dims(latent_rep, axis=1),\n",
        "                         [1, num_targets, 1])\n",
        "        \n",
        "    if self._use_deterministic_path:\n",
        "      deterministic_rep = self._deterministic_encoder(context_x, context_y,\n",
        "                                                      target_x)\n",
        "\n",
        "      if self._use_encoder_latent_cross_attention:    \n",
        "        representation = tf.concat([deterministic_rep, latent_cross, latent_rep], axis=-1)\n",
        "      else:\n",
        "        representation = tf.concat([deterministic_rep,latent_rep],axis=-1)\n",
        "        \n",
        "    else:\n",
        "      if self._use_encoder_latent_cross_attention:\n",
        "        representation = tf.concat([latent_cross, latent_rep], axis=-1)\n",
        "      else:\n",
        "        representation = latent_rep\n",
        "      \n",
        "    dist, mu, sigma = self._decoder(representation, target_x)\n",
        "    \n",
        "    # If we want to calculate the log_prob for training we will make use of the\n",
        "    # target_y. At test time the target_y is not available so we return None.\n",
        "    if target_y is not None:\n",
        "      log_p = dist.log_prob(target_y)\n",
        "      posterior = self._latent_encoder(target_x, target_y)\n",
        "      \n",
        "      kl = tf.reduce_sum(\n",
        "          tf.contrib.distributions.kl_divergence(posterior, prior), \n",
        "          axis=-1, keepdims=True)\n",
        "      kl = tf.tile(kl, [1, num_targets])\n",
        "      \n",
        "#       # TODO: KL for cross_attention part of latent encoder\n",
        "#       kl_cross = tf.reduce_sum(\n",
        "#           tf.contrib.distributions.kl_divergence(z_samps_posterior, z_samps_prior), \n",
        "#           axis=-1, keepdims=True)\n",
        "#       kl_cross = tf.tile(kl_cross, [1, num_targets])\n",
        "      \n",
        "      loss = - tf.reduce_mean(log_p - kl / tf.cast(num_targets, tf.float32))\n",
        "    else:\n",
        "      log_p = None\n",
        "      kl = None\n",
        "      loss = None\n",
        "\n",
        "    return mu, sigma, log_p, kl, loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULaa6hZEuHvU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross-Attention Module\n",
        "Given a set of key-value pairs $(k_i,v_i)_{i \\in I}$ and query $q$, an attention module computes weights for each key and aggregates the values with these weights to form the value corresponding to the query.\n",
        "\n",
        "\n",
        "**`rep`** determines whether the raw inputs to the module will be used as the keys and queries, or whether you will pass them through an MLP and use the output instead. One of 'identity', 'mlp'.\n",
        "\n",
        "**`output_sizes`** determines the architecture of the MLP used to obtain the keys/queries if `rep` is 'mlp'.\n",
        "\n",
        "**`att_type`** is a string argument that determines the type of attention used. Valid choices of attention are: uniform, laplace, dot product, multihead.\n",
        "\n",
        "* **Uniform** $((k_i,v_i)_{i\\in I}, q)= \\frac{1}{|I|} \\sum_i v_i$\n",
        "\n",
        "* **Laplace** $((k_i,v_i)_{i\\in I}, q)= \\sum_i w_i v_i, \\hspace{2mm} w_i \\propto \\exp(-\\frac{||q - k_i||_1}{l})$\n",
        "\n",
        "* **DotProduct** $((k_i,v_i)_{i\\in I}, q)= \\sum_i w_i v_i, \\hspace{2mm} w_i \\propto \\exp(q^\\top k_i / \\sqrt{d_k})$ where $k_i \\in \\mathbb{R}^{d_k}$.\n",
        "\n",
        "* **Multihead** $((k_i,v_i)_{i\\in I}, q)= \\mathcal{L}^O(\\text{concat}(\\text{head}_1, \\ldots, \\text{head}_H))$, $\\text{head}_h = \\text{DotProduct}((\\mathcal{L}^K_h(k_i),\\mathcal{L}^V_h(v_i))_{i \\in I}, \\mathcal{L}^Q_h(q))$ \n",
        "\n",
        "  where $\\mathcal{L}$ are linear maps with trainable parameters.\n",
        "\n",
        "**`scale`**: length scale $l$ in Laplace attention.\n",
        "\n",
        "**`normalise`**: whether to use a softmax so that weights sum to 1 or not.\n",
        "\n",
        "**`num_heads`**: $H$, the number of heads for multihead attention."
      ]
    },
    {
      "metadata": {
        "id": "DImJP8HfhmmM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def uniform_attention(q, v):\n",
        "  \"\"\"Uniform attention. Equivalent to np.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of shape [B,m,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  total_points = tf.shape(q)[1]\n",
        "  rep = tf.reduce_mean(v, axis=1, keepdims=True)  # [B,1,d_v]\n",
        "  rep = tf.tile(rep, [1, total_points, 1])\n",
        "  return rep\n",
        "\n",
        "def laplace_attention(q, k, v, scale, normalise):\n",
        "  \"\"\"Computes laplace exponential attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    scale: float that scales the L1 distance.\n",
        "    normalise: Boolean that determines whether weights sum to 1.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  k = tf.expand_dims(k, axis=1)  # [B,1,n,d_k]\n",
        "  q = tf.expand_dims(q, axis=2)  # [B,m,1,d_k]\n",
        "  unnorm_weights = - tf.abs((k - q) / scale)  # [B,m,n,d_k]\n",
        "  unnorm_weights = tf.reduce_sum(unnorm_weights, axis=-1)  # [B,m,n]\n",
        "  if normalise:\n",
        "    weight_fn = tf.nn.softmax\n",
        "  else:\n",
        "    weight_fn = lambda x: 1 + tf.tanh(x)\n",
        "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
        "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
        "  return rep\n",
        "\n",
        "\n",
        "def dot_product_attention(q, k, v, normalise):\n",
        "  \"\"\"Computes dot product attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of  shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    normalise: Boolean that determines whether weights sum to 1.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  #print('I hate it')\n",
        "  d_k = tf.shape(q)[-1]\n",
        "  scale = tf.sqrt(tf.cast(d_k, tf.float32))\n",
        "  unnorm_weights = tf.einsum('bjk,bik->bij', k, q) / scale  # [B,m,n]\n",
        "  if normalise:\n",
        "    weight_fn = tf.nn.softmax\n",
        "  else:\n",
        "    weight_fn = tf.sigmoid\n",
        "  weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
        "  rep = tf.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
        "  return rep\n",
        "\n",
        "\n",
        "def multihead_attention(q, k, v, num_heads=8):\n",
        "  \"\"\"Computes multi-head attention.\n",
        "\n",
        "  Args:\n",
        "    q: queries. tensor of  shape [B,m,d_k].\n",
        "    k: keys. tensor of shape [B,n,d_k].\n",
        "    v: values. tensor of shape [B,n,d_v].\n",
        "    num_heads: number of heads. Should divide d_v.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,m,d_v].\n",
        "  \"\"\"\n",
        "  d_k = q.get_shape().as_list()[-1]\n",
        "  d_v = v.get_shape().as_list()[-1]\n",
        "  head_size = d_v / num_heads\n",
        "  key_initializer = tf.random_normal_initializer(stddev=d_k**-0.5)\n",
        "  value_initializer = tf.random_normal_initializer(stddev=d_v**-0.5)\n",
        "  rep = tf.constant(0.0)\n",
        "  for h in range(num_heads):\n",
        "    o = dot_product_attention(\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wq%d' % h, use_bias=False, padding='VALID')(q),\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wk%d' % h, use_bias=False, padding='VALID')(k),\n",
        "        tf.layers.Conv1D(head_size, 1, kernel_initializer=key_initializer,\n",
        "                   name='wv%d' % h, use_bias=False, padding='VALID')(v),\n",
        "        normalise=True)\n",
        "    rep += tf.layers.Conv1D(d_v, 1, kernel_initializer=value_initializer,\n",
        "                      name='wo%d' % h, use_bias=False, padding='VALID')(o)\n",
        "  return rep\n",
        "\n",
        "class Attention(object):\n",
        "  \"\"\"The Attention module.\"\"\"\n",
        "\n",
        "  def __init__(self, rep, output_sizes, att_type, scale=1., normalise=True,\n",
        "               num_heads=8):\n",
        "    \"\"\"Create attention module.\n",
        "\n",
        "    Takes in context inputs, target inputs and\n",
        "    representations of each context input/output pair\n",
        "    to output an aggregated representation of the context data.\n",
        "    Args:\n",
        "      rep: transformation to apply to contexts before computing attention. \n",
        "          One of: ['identity','mlp'].\n",
        "      output_sizes: list of number of hidden units per layer of mlp.\n",
        "          Used only if rep == 'mlp'.\n",
        "      att_type: type of attention. One of the following:\n",
        "          ['uniform','laplace','dot_product','multihead']\n",
        "      scale: scale of attention.\n",
        "      normalise: Boolean determining whether to:\n",
        "          1. apply softmax to weights so that they sum to 1 across context pts or\n",
        "          2. apply custom transformation to have weights in [0,1].\n",
        "      num_heads: number of heads for multihead.\n",
        "    \"\"\"\n",
        "    self._rep = rep\n",
        "    self._output_sizes = output_sizes\n",
        "    self._type = att_type\n",
        "    self._scale = scale\n",
        "    self._normalise = normalise\n",
        "    if self._type == 'multihead':\n",
        "      self._num_heads = num_heads\n",
        "\n",
        "  def __call__(self, x1, x2, r):\n",
        "    \"\"\"Apply attention to create aggregated representation of r.\n",
        "\n",
        "    Args:\n",
        "      x1: tensor of shape [B,n1,d_x].\n",
        "      x2: tensor of shape [B,n2,d_x].\n",
        "      r: tensor of shape [B,n1,d].\n",
        "      \n",
        "    Returns:\n",
        "      tensor of shape [B,n2,d]\n",
        "\n",
        "    Raises:\n",
        "      NameError: The argument for rep/type was invalid.\n",
        "    \"\"\"\n",
        "    if self._rep == 'identity':\n",
        "      k, q = (x1, x2)\n",
        "    elif self._rep == 'mlp':\n",
        "      # Pass through MLP\n",
        "      k = batch_mlp(x1, self._output_sizes, \"attention\")\n",
        "      q = batch_mlp(x2, self._output_sizes, \"attention\")\n",
        "    else:\n",
        "      raise NameError(\"'rep' not among ['identity','mlp']\")\n",
        "\n",
        "    if self._type == 'uniform':\n",
        "      rep = uniform_attention(q, r)\n",
        "    elif self._type == 'laplace':\n",
        "      rep = laplace_attention(q, k, r, self._scale, self._normalise)\n",
        "    elif self._type == 'dot_product':\n",
        "      rep = dot_product_attention(q, k, r, self._normalise)\n",
        "    elif self._type == 'multihead':\n",
        "      rep = multihead_attention(q, k, r, self._num_heads)\n",
        "    else:\n",
        "      raise NameError((\"'att_type' not among ['uniform','laplace','dot_product'\"\n",
        "                       \",'multihead']\"))\n",
        "\n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zYcTQDLltFmA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ploting function\n",
        "\n",
        "Same plotting function as for the [implementation of CNPs](https://github.com/deepmind/conditional-neural-process/blob/master/conditional_neural_process.ipynb) that plots the intermediate predictions every so often during training."
      ]
    },
    {
      "metadata": {
        "id": "AFlT3lJQTM_m",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def plot_functions(target_x, target_y, context_x, context_y, pred_y, std):\n",
        "  \"\"\"Plots the predicted mean and variance and the context points.\n",
        "  \n",
        "  Args: \n",
        "    target_x: An array of shape [B,num_targets,1] that contains the\n",
        "        x values of the target points.\n",
        "    target_y: An array of shape [B,num_targets,1] that contains the\n",
        "        y values of the target points.\n",
        "    context_x: An array of shape [B,num_contexts,1] that contains \n",
        "        the x values of the context points.\n",
        "    context_y: An array of shape [B,num_contexts,1] that contains \n",
        "        the y values of the context points.\n",
        "    pred_y: An array of shape [B,num_targets,1] that contains the\n",
        "        predicted means of the y values at the target points in target_x.\n",
        "    std: An array of shape [B,num_targets,1] that contains the\n",
        "        predicted std dev of the y values at the target points in target_x.\n",
        "  \"\"\"\n",
        "  # Plot everything\n",
        "  plt.plot(target_x[0], pred_y[0], 'b', linewidth=2)\n",
        "  plt.plot(target_x[0], target_y[0], 'k:', linewidth=2)\n",
        "  plt.plot(context_x[0], context_y[0], 'ko', markersize=10)\n",
        "  plt.fill_between(\n",
        "      target_x[0, :, 0],\n",
        "      pred_y[0, :, 0] - std[0, :, 0],\n",
        "      pred_y[0, :, 0] + std[0, :, 0],\n",
        "      alpha=0.2,\n",
        "      facecolor='#65c9f7',\n",
        "      interpolate=True)\n",
        "\n",
        "  # Make the plot pretty\n",
        "  plt.yticks([-2, 0, 2], fontsize=16)\n",
        "  plt.xticks([-2, 0, 2], fontsize=16)\n",
        "  plt.ylim([-2, 2])\n",
        "  plt.grid('off')\n",
        "  ax = plt.gca()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z69Ham1nteeD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the (A)NP\n",
        "\n",
        "We can now start training. First we need to define some variables:\n",
        "\n",
        "**`TRAINING_ITERATIONS`**: Number of iterations used for trianing. At each iteration we sample a new batch of sample curves from GPs and pick a random set of points on each curve to be the target and a subset to be the context. We optimise the ELBO on the log predictive likelihood of the target given context.\n",
        "\n",
        "**`MAX_CONTEXT_POINTS`**: Maximum number of context points used during training. This is also set to be the upper bound on the number of target points.\n",
        "\n",
        "**`PLOT_AFTER`**: The number of iterations between the intermediate plots.\n",
        "\n",
        "**`HIDDEN_SIZE`**: Master parameter that governs the hidden layer size of all MLPs in the model and also the latent dimensionality. \n",
        "\n",
        "**`MODEL_TYPE`**: 'NP' or 'ANP'.\n",
        "\n",
        "**`ATTENTION_TYPE`**: The type of attention used for ANP. One of `uniform`, `laplace` `dot_product` or `multihead`\n",
        "\n",
        "**`random_kernel_parameters`**: Boolean to determine whether the GP kernel parameters are sample randomly for each iteration or fixed.\n"
      ]
    },
    {
      "metadata": {
        "id": "yHqnv8FP4Rtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### NP training\n",
        "First we train the NP. Notice from the plots that the predictions after 1e5 iterations do not go through the context points perfectly - i.e. underfits."
      ]
    },
    {
      "metadata": {
        "id": "c6FcSLfnLD9_",
        "colab_type": "code",
        "outputId": "ba763990-e51f-4acc-bfda-818ba55589f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''TRAINING_ITERATIONS = 100000 #@param {type:\"number\"}\n",
        "MAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\n",
        "PLOT_AFTER = 10000 #@param {type:\"number\"}\n",
        "HIDDEN_SIZE = 128 #@param {type:\"number\"}\n",
        "MODEL_TYPE = 'NP' #@param ['NP','ANP']\n",
        "ATTENTION_TYPE = 'dot_product' #@param ['uniform','laplace','dot_product','multihead']\n",
        "random_kernel_parameters=True #@param {type:\"boolean\"}\n",
        "\n",
        "tf.reset_default_graph()\n",
        "# Train dataset\n",
        "dataset_train = GPCurvesReader(\n",
        "    batch_size=16, max_num_context=MAX_CONTEXT_POINTS, random_kernel_parameters=random_kernel_parameters)\n",
        "data_train = dataset_train.generate_curves()\n",
        "\n",
        "# Test dataset\n",
        "dataset_test = GPCurvesReader(\n",
        "    batch_size=1, max_num_context=MAX_CONTEXT_POINTS, testing=True, random_kernel_parameters=random_kernel_parameters)\n",
        "data_test = dataset_test.generate_curves()\n",
        "\n",
        "\n",
        "# Sizes of the layers of the MLPs for the encoders and decoder\n",
        "# The final output layer of the decoder outputs two values, one for the mean and\n",
        "# one for the variance of the prediction at the target location\n",
        "latent_encoder_output_sizes = [HIDDEN_SIZE]*4\n",
        "num_latents = HIDDEN_SIZE\n",
        "deterministic_encoder_output_sizes= [HIDDEN_SIZE]*4\n",
        "decoder_output_sizes = [HIDDEN_SIZE]*2 + [2]\n",
        "use_deterministic_path = True\n",
        "\n",
        "\n",
        "# ANP with multihead attention\n",
        "if MODEL_TYPE == 'ANP':\n",
        "  attention = Attention(rep='mlp', output_sizes=[HIDDEN_SIZE]*2, \n",
        "                        att_type=ATTENTION_TYPE)\n",
        "# NP - equivalent to uniform attention\n",
        "elif MODEL_TYPE == 'NP':\n",
        "  attention = Attention(rep='identity', output_sizes=None, att_type='uniform')\n",
        "else:\n",
        "  raise NameError(\"MODEL_TYPE not among ['ANP,'NP']\")\n",
        "\n",
        "# Define the model\n",
        "model = LatentModel(latent_encoder_output_sizes, num_latents,\n",
        "                    decoder_output_sizes, use_deterministic_path, \n",
        "                    deterministic_encoder_output_sizes, attention)\n",
        "\n",
        "# Define the loss\n",
        "_, _, log_prob, _, loss = model(data_train.query, data_train.num_total_points,\n",
        "                                 data_train.target_y)\n",
        "\n",
        "# Get the predicted mean and variance at the target points for the testing set\n",
        "mu, sigma, _, _, _ = model(data_test.query, data_test.num_total_points)\n",
        "\n",
        "# Set up the optimizer and train step\n",
        "optimizer = tf.train.AdamOptimizer(1e-4)\n",
        "train_step = optimizer.minimize(loss)\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Train and plot\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  for it in range(TRAINING_ITERATIONS):\n",
        "    sess.run([train_step])\n",
        "\n",
        "    # Plot the predictions in `PLOT_AFTER` intervals\n",
        "    if it % PLOT_AFTER == 0:\n",
        "      loss_value, pred_y, std_y, target_y, whole_query = sess.run(\n",
        "          [loss, mu, sigma, data_test.target_y, \n",
        "           data_test.query])\n",
        "\n",
        "      (context_x, context_y), target_x = whole_query\n",
        "      print('Iteration: {}, loss: {}'.format(it, loss_value))\n",
        "\n",
        "      # Plot the prediction and the context\n",
        "      plot_functions(target_x, target_y, context_x, context_y, pred_y, std_y)'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TRAINING_ITERATIONS = 100000 #@param {type:\"number\"}\\nMAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\\nPLOT_AFTER = 10000 #@param {type:\"number\"}\\nHIDDEN_SIZE = 128 #@param {type:\"number\"}\\nMODEL_TYPE = \\'NP\\' #@param [\\'NP\\',\\'ANP\\']\\nATTENTION_TYPE = \\'dot_product\\' #@param [\\'uniform\\',\\'laplace\\',\\'dot_product\\',\\'multihead\\']\\nrandom_kernel_parameters=True #@param {type:\"boolean\"}\\n\\ntf.reset_default_graph()\\n# Train dataset\\ndataset_train = GPCurvesReader(\\n    batch_size=16, max_num_context=MAX_CONTEXT_POINTS, random_kernel_parameters=random_kernel_parameters)\\ndata_train = dataset_train.generate_curves()\\n\\n# Test dataset\\ndataset_test = GPCurvesReader(\\n    batch_size=1, max_num_context=MAX_CONTEXT_POINTS, testing=True, random_kernel_parameters=random_kernel_parameters)\\ndata_test = dataset_test.generate_curves()\\n\\n\\n# Sizes of the layers of the MLPs for the encoders and decoder\\n# The final output layer of the decoder outputs two values, one for the mean and\\n# one for the variance of the prediction at the target location\\nlatent_encoder_output_sizes = [HIDDEN_SIZE]*4\\nnum_latents = HIDDEN_SIZE\\ndeterministic_encoder_output_sizes= [HIDDEN_SIZE]*4\\ndecoder_output_sizes = [HIDDEN_SIZE]*2 + [2]\\nuse_deterministic_path = True\\n\\n\\n# ANP with multihead attention\\nif MODEL_TYPE == \\'ANP\\':\\n  attention = Attention(rep=\\'mlp\\', output_sizes=[HIDDEN_SIZE]*2, \\n                        att_type=ATTENTION_TYPE)\\n# NP - equivalent to uniform attention\\nelif MODEL_TYPE == \\'NP\\':\\n  attention = Attention(rep=\\'identity\\', output_sizes=None, att_type=\\'uniform\\')\\nelse:\\n  raise NameError(\"MODEL_TYPE not among [\\'ANP,\\'NP\\']\")\\n\\n# Define the model\\nmodel = LatentModel(latent_encoder_output_sizes, num_latents,\\n                    decoder_output_sizes, use_deterministic_path, \\n                    deterministic_encoder_output_sizes, attention)\\n\\n# Define the loss\\n_, _, log_prob, _, loss = model(data_train.query, data_train.num_total_points,\\n                                 data_train.target_y)\\n\\n# Get the predicted mean and variance at the target points for the testing set\\nmu, sigma, _, _, _ = model(data_test.query, data_test.num_total_points)\\n\\n# Set up the optimizer and train step\\noptimizer = tf.train.AdamOptimizer(1e-4)\\ntrain_step = optimizer.minimize(loss)\\ninit = tf.initialize_all_variables()\\n\\n# Train and plot\\nwith tf.Session() as sess:\\n  sess.run(init)\\n\\n  for it in range(TRAINING_ITERATIONS):\\n    sess.run([train_step])\\n\\n    # Plot the predictions in `PLOT_AFTER` intervals\\n    if it % PLOT_AFTER == 0:\\n      loss_value, pred_y, std_y, target_y, whole_query = sess.run(\\n          [loss, mu, sigma, data_test.target_y, \\n           data_test.query])\\n\\n      (context_x, context_y), target_x = whole_query\\n      print(\\'Iteration: {}, loss: {}\\'.format(it, loss_value))\\n\\n      # Plot the prediction and the context\\n      plot_functions(target_x, target_y, context_x, context_y, pred_y, std_y)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "pudKobiUJbbT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ANP training\n",
        "\n",
        "Next, we train the ANP with multihead attention, using the same hyperparameter setting as NPs. Note that the predictions are now much more accurate for the observed context data."
      ]
    },
    {
      "metadata": {
        "id": "q6wQwAwjTEc_",
        "colab_type": "code",
        "outputId": "b6fddf7b-a55a-4454-e0d9-ebfd22f5e07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2322
        }
      },
      "cell_type": "code",
      "source": [
        "TRAINING_ITERATIONS = 30000 #@param {type:\"number\"}\n",
        "MAX_CONTEXT_POINTS = 50 #@param {type:\"number\"}\n",
        "PLOT_AFTER = 10000 #@param {type:\"number\"}\n",
        "HIDDEN_SIZE = 128 #@param {type:\"number\"}\n",
        "MODEL_TYPE = 'ANP' #@param ['NP','ANP']\n",
        "ATTENTION_TYPE = 'dot_product' #@param ['uniform','laplace','dot_product','multihead']\n",
        "random_kernel_parameters=True #@param {type:\"boolean\"}\n",
        "use_decoder_self_attention = True #@param {type:\"boolean\"}\n",
        "use_encoder_determ_self_attention = True #@param {type:\"boolean\"}\n",
        "use_encoder_latent_self_attention = True #@param {type:\"boolean\"}\n",
        "use_encoder_latent_cross_attention = True #@param {type:\"boolean\"}\n",
        "\n",
        "filename = 'loss_arr_{}_{}_{}_{}.npy'.format(use_decoder_self_attention,\n",
        "                                            use_encoder_determ_self_attention,\n",
        "                                            use_encoder_latent_self_attention,\n",
        "                                            use_encoder_latent_cross_attention)\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "# Train dataset\n",
        "dataset_train = GPSinCurvesReader(\n",
        "    batch_size=16, max_num_context=MAX_CONTEXT_POINTS)\n",
        "data_train = dataset_train.generate_curves()\n",
        "\n",
        "# Test dataset\n",
        "dataset_test = GPSinCurvesReader(\n",
        "    batch_size=1, max_num_context=MAX_CONTEXT_POINTS, testing=True)\n",
        "data_test = dataset_test.generate_curves()\n",
        "\n",
        "\n",
        "# Sizes of the layers of the MLPs for the encoders and decoder\n",
        "# The final output layer of the decoder outputs two values, one for the mean and\n",
        "# one for the variance of the prediction at the target location\n",
        "latent_encoder_output_sizes = [HIDDEN_SIZE]*4\n",
        "num_latents = HIDDEN_SIZE\n",
        "deterministic_encoder_output_sizes= [HIDDEN_SIZE]*4\n",
        "decoder_output_sizes = [HIDDEN_SIZE]*2 + [2]\n",
        "use_deterministic_path = True\n",
        "\n",
        "# ANP with multihead attention\n",
        "if MODEL_TYPE == 'ANP':\n",
        "  attention = Attention(rep='mlp', output_sizes=[HIDDEN_SIZE]*2, \n",
        "                        att_type='multihead')\n",
        "# NP - equivalent to uniform attention\n",
        "elif MODEL_TYPE == 'NP':\n",
        "  attention = Attention(rep='identity', output_sizes=None, att_type='uniform')\n",
        "else:\n",
        "  raise NameError(\"MODEL_TYPE not among ['ANP,'NP']\")\n",
        "\n",
        "# Define the model\n",
        "model = LatentModel(latent_encoder_output_sizes, num_latents,\n",
        "                    decoder_output_sizes, use_deterministic_path, \n",
        "                    deterministic_encoder_output_sizes, attention, \n",
        "                    use_attention_decoder = use_decoder_self_attention,\n",
        "                    use_encoder_determ_self_attention = use_encoder_determ_self_attention,\n",
        "                    use_encoder_latent_self_attention = use_encoder_latent_self_attention,\n",
        "                    use_encoder_latent_cross_attention = use_encoder_latent_cross_attention)\n",
        "\n",
        "# Define the loss\n",
        "_, _, log_prob, _, loss = model(data_train.query, data_train.num_total_points,\n",
        "                                 data_train.target_y)\n",
        "\n",
        "# Get the predicted mean and variance at the target points for the testing set\n",
        "mu, sigma, _, _, _ = model(data_test.query, data_test.num_total_points)\n",
        "\n",
        "# Set up the optimizer and train step\n",
        "optimizer = tf.train.AdamOptimizer(1e-4)\n",
        "train_step = optimizer.minimize(loss)\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# set up loss array\n",
        "loss_arr = np.zeros(TRAINING_ITERATIONS)\n",
        "\n",
        "# Train and plot\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  for it in range(TRAINING_ITERATIONS):\n",
        "    sess.run([train_step])\n",
        "\n",
        "    # Plot the predictions in `PLOT_AFTER` intervals\n",
        "    if it % PLOT_AFTER == 0:\n",
        "      loss_value, pred_y, std_y, target_y, whole_query = sess.run(\n",
        "          [loss, mu, sigma, data_test.target_y, \n",
        "           data_test.query])     \n",
        "      \n",
        "\n",
        "      (context_x, context_y), target_x = whole_query\n",
        "      print('Iteration: {}, loss: {}'.format(it, loss_value))\n",
        "\n",
        "      # Plot the prediction and the context\n",
        "      plot_functions(target_x, target_y, context_x, context_y, pred_y, std_y)\n",
        "      \n",
        "    if it%PLOT_AFTER == 0:\n",
        "      np.save(filename,loss_arr)\n",
        "    loss_arr[it] = loss_value\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cross attention in the latent encoder \n",
            "Decoder Attention is  True\n",
            "Using self attention in the latent encoder\n",
            "WARNING:tensorflow:From <ipython-input-4-e7eba148c7b5>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-6-84255382ec82>:60: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "Using self attention in the latent cross encoder\n",
            "Using self attention in the latent encoder\n",
            "Using self attention in the latent cross encoder\n",
            "Uaing self attention in the deterministic encoder\n",
            "Using self attention in the decoder\n",
            "Input Hidden  Tensor(\"Reshape_25:0\", shape=(16, ?, 128), dtype=float32)\n",
            "Attention Hidden  Tensor(\"hidden_decoder_1/add_7:0\", shape=(16, ?, 128), dtype=float32)\n",
            "Output hidden  Tensor(\"Reshape_27:0\", shape=(16, ?, 2), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-8-77482f2865ab>:67: MultivariateNormalDiag.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_diag) is deprecated and will be removed after 2018-10-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/mvn_diag.py:224: MultivariateNormalLinearOperator.__init__ (from tensorflow.contrib.distributions.python.ops.mvn_linear_operator) is deprecated and will be removed after 2018-10-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:201: AffineLinearOperator.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.affine_linear_operator) is deprecated and will be removed after 2018-10-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/bijectors/affine_linear_operator.py:158: _DistributionShape.__init__ (from tensorflow.contrib.distributions.python.ops.shape) is deprecated and will be removed after 2018-10-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/distributions/python/ops/mvn_linear_operator.py:205: TransformedDistribution.__init__ (from tensorflow.python.ops.distributions.transformed_distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "Using self attention in the latent encoder\n",
            "WARNING:tensorflow:From <ipython-input-9-e0e6fc1dc9d9>:109: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "Using self attention in the latent encoder\n",
            "Using self attention in the latent cross encoder\n",
            "Uaing self attention in the deterministic encoder\n",
            "Using self attention in the decoder\n",
            "Input Hidden  Tensor(\"Reshape_47:0\", shape=(1, 400, 128), dtype=float32)\n",
            "Attention Hidden  Tensor(\"hidden_decoder_3/add_7:0\", shape=(1, 400, 128), dtype=float32)\n",
            "Output hidden  Tensor(\"Reshape_49:0\", shape=(1, 400, 2), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "Iteration: 0, loss: 0.8409408330917358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
            "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
            "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFVCAYAAADR+vcXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 10000, loss: 0.6341500282287598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFVCAYAAADR+vcXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 20000, loss: -0.4127345383167267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFVCAYAAADR+vcXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgghIkFBmRBCCBGJ/wNvGXRabEgErwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XLbGNC_bXaUx",
        "colab_type": "code",
        "outputId": "aa7ce5a2-cf01-4234-c542-f420faf9edcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "cell_type": "code",
      "source": [
        "loss_arr[29999]\n",
        "np.save('loss_sin_changes',loss_arr)\n",
        "files.download('loss_sin_changes.npy')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1b1d878d88da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m29999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss_sin_changes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss_sin_changes.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rz-TD-ss9gWx",
        "colab_type": "code",
        "outputId": "ac2c086e-9d31-42a6-f1f9-b9e6bff07fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(loss_arr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}